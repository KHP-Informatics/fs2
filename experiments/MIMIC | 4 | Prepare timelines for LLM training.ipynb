{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efa953",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"/data/zeljko/projects/medgpt/\")\n",
    "sys.path.insert(0, \"/data/zeljko/projects/MedCAT/\")\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/data/zeljko/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/data/zeljko/.cache/huggingface\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2bc46",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from medcat.cat import CAT\n",
    "from datasets import DatasetDict\n",
    "from medgpt.datasets import patient_concept_stream\n",
    "from medgpt.datasets.filters import filter_by_count, filter_by_type\n",
    "from medgpt.datasets.utils import get_embeddings_for_tokens, stream_to_separate_examples, add_to_stream, \\\n",
    "                                  remove_parents_from_stream, bucket_concepts, cleanup_stream, \\\n",
    "                                  split_stream, add_age, get_all_splits, add_ttd, add_position_ids, \\\n",
    "                                  fix_types_for_presence\n",
    "from medgpt.utils.cdb_utils import get_parents_map \n",
    "from medgpt.utils.stream_utils import docs2stream, get_patient_count_per_token, get_token_counts_from_dataset\n",
    "from medgpt.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "from medgpt.tokenizers.utils import encode_stream\n",
    "from medgpt.metrics.next_concept_prediction import precision, metrics_data2df, ComputePrecisionHF\n",
    "from medcat.cdb import CDB\n",
    "from medgpt.utils import pickle\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer, pipeline, GPT2Tokenizer, LlamaTokenizerFast, LlamaTokenizer\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "from medgpt.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a180d92-9694-43d3-9ef1-47afc6ebf81c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(yaml_path='/home/ubuntu/projects/medgpt/configs/mimic-mistral.yaml', \n",
    "                extra_yaml_paths=['/home/ubuntu/projects/medgpt/configs/mimic-seq-len-4096.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856508f-7d6a-41a0-a7dc-f0964d603d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.path.dataset.hf_output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a6109",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "FORCE = False # If true a lot of things will be rebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f7e3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(config.train.device)\n",
    "# This is internal config, only for this notebook\n",
    "BATCH_SIZE = 1000\n",
    "NUM_PROC = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a7fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = CAT.load_model_pack(config.path.cat, meta_cat_config_dict={'general': {'device': config.cat.meta.device}})\n",
    "cdb = cat.cdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211cac5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2info = pickle.load(open(config.path.dataset.doc2info, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58f3f6",
   "metadata": {},
   "source": [
    "### Get counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f41ec3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_paths = [os.path.join(config.path.dataset.annotated_documents, path) for path in os.listdir(config.path.dataset.annotated_documents) \n",
    "              if path.startswith(\"part_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6d471",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt2cui2cnt = get_token_counts_from_dataset(\n",
    "                 doc_paths=doc_paths,\n",
    "                 doc2info=doc2info,\n",
    "                 meta_requirements={'Subject': 'Patient'}, \n",
    "                 save_path=config.path.dataset.pt2cui2cnt,\n",
    "                 force=False)\n",
    "len(pt2cui2cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abce7b5",
   "metadata": {},
   "source": [
    "### Get pt2stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01cb22-1e3a-4341-9604-fd9318c30b68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2text = pickle.load(open(config.path.dataset.doc2text, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a75cc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit, Split, Sequence\n",
    "from tokenizers import Regex\n",
    "\n",
    "#pu = Split(Regex(r'[.;:!?\\n]+'), behavior='isolated')\n",
    "ws = Split(Regex(r'[$ ]*[^ \\n]+[\\n]*'), behavior='isolated')\n",
    "pre_tokenizer = Sequence([ws]) # Only space, ignore everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657cc56-d7f7-4082-973e-9e7073060d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"I was. - \\n\\nrunning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a10604-e7db-4d36-ad52-f77bc6b69bca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt2stream = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f85078",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt2stream = docs2stream(doc_paths,\n",
    "                        doc2info=doc2info,\n",
    "                        pt2cui2cnt=pt2cui2cnt,\n",
    "                        entity_type_column='type_ids',\n",
    "                        meta_requirements={'Subject': 'Patient'}, # Presence will be an option to filter by later\n",
    "                        historical_meta=None,\n",
    "                        skip_cuis={'S-418023006', '17971005'},\n",
    "                        require_time=True,\n",
    "                        save_path=config.path.dataset.self,\n",
    "                        tokenizer=pre_tokenizer.pre_tokenize_str,\n",
    "                        doc2text=doc2text,\n",
    "                        force=False,\n",
    "                        cntx_size=config.train.cntx_size,\n",
    "                        sentence_limits=tuple(config.train.sentence_limits) if 'sentence_limits' in config.train and config.train.sentence_limits else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2cd63-f28e-4f86-955c-88330de5e51e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cui_by_pt = get_patient_count_per_token(pt2stream, force=False, save_path=config.path.dataset.cui_by_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894ceb1",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5399b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(os.path.abspath(patient_concept_stream.__file__), data_files=[config.path.dataset.self])['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665081bf-352c-4dd9-b1eb-64ad06f56bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run unless you are testing stuff\n",
    "import random\n",
    "#from datasets import Dataset\n",
    "#inds = random.sample([i for i in range(len(dataset))], k=200)\n",
    "#dataset = Dataset.from_dict(dataset[inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79bc70",
   "metadata": {},
   "source": [
    "### Filter by count, split and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21640d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_ids_test_set = set([str(x) for x in pd.read_csv(config.path.dataset.test_df).subject_id.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064765f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = filter_by_count(dataset, \n",
    "                          min_count=config.train.min_count, \n",
    "                          min_count_global=config.train.min_global_count, \n",
    "                          min_length=config.train.min_length, \n",
    "                          max_length=-1, \n",
    "                          num_proc=NUM_PROC, \n",
    "                          token_cnt=cui_by_pt)\n",
    "#dataset = dataset.train_test_split(test_size = 0.05)\n",
    "train_ds = dataset.filter(lambda example: example['patient_id'] not in patient_ids_test_set,\n",
    "                          num_proc=NUM_PROC)\n",
    "test_ds = dataset.filter(lambda example: example['patient_id'] in patient_ids_test_set,\n",
    "                         num_proc=NUM_PROC)\n",
    "dataset = DatasetDict({'train': train_ds, 'test': test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccea34-3adb-4272-af53-04b0cd0d1d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train': train_ds, 'test': test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba94637-7286-452f-88a5-e718034ad0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(config.path.dataset.splits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412836b3",
   "metadata": {},
   "source": [
    "### Bucket examples and remove parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794b5db-5604-41fb-9565-41eaa2380b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.splits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c169e-b43c-4098-b056-501f2e663b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]['stream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef6c4b-8918-4932-91a6-9447917229aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need to remove parents early on, because it can messup other things like temporality \n",
    "cuis = pickle.load(open(config.path.dataset.cuis_in_text, 'rb'))\n",
    "ch2parents = get_parents_map(cuis, cdb.addl_info['pt2ch'], depth=2)\n",
    "dataset = dataset.map(\n",
    "        lambda examples: remove_parents_from_stream(examples, ch2parents=ch2parents, separator=None),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb137110",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda examples: bucket_concepts(examples, bucket_size_seconds=config.train.days*24*60*60, time_prefix=''), #'<TIME> '), # Requires a space at the end\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac517f6-14f1-41b3-8588-54684824aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt2stream = None\n",
    "cui_by_pt = get_patient_count_per_token(pt2stream, force=False, save_path=config.path.dataset.cui_by_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e16fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trim timelines longer than MAX_LEN\n",
    "dataset = filter_by_count(dataset, min_count=0, min_count_global=0, \n",
    "                          min_length=config.train.min_length, \n",
    "                          max_length=8*config.train.max_timeline_len, # This is just to prevent some timelines from being ultra long, also this is timelines in concepts, never happens that they are this long\n",
    "                          num_proc=NUM_PROC, \n",
    "                          token_cnt=cui_by_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd560f9d-cf42-47dc-857c-689ab8144ffb",
   "metadata": {},
   "source": [
    "### Change token type to match presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905493f-3247-4bdb-a6d8-5f77a562485a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda examples: fix_types_for_presence(examples, config.train.token_type_prefix),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f278f",
   "metadata": {},
   "source": [
    "### Add demographics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e0906",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt2info = pickle.load(open(config.path.dataset.pt2info, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5b654",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Sex\n",
    "dataset = dataset.map(\n",
    "        lambda examples: add_to_stream(examples, pt2info, last=False, prefix=None, key='Sex', token_type='sex', lowercase=False),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43094fe8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Eth\n",
    "dataset = dataset.map(\n",
    "        lambda examples: add_to_stream(examples, pt2info, last=False, prefix=None, key='eth', token_type='ethnicity', lowercase=True),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bf847",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda examples: add_age(examples, pt2info=pt2info),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc95d5e",
   "metadata": {},
   "source": [
    "### Add start and end tokens `<s> </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f1a9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add <s>\n",
    "dataset = dataset.map(\n",
    "        lambda examples: add_to_stream(examples, one_token=config.tokenizer.special_tokens.bos_token, \n",
    "                                       token_type='bos_token', add_space=False),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c8f2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add </s>\n",
    "dataset = dataset.map(\n",
    "        lambda examples: add_to_stream(examples, one_token=config.tokenizer.special_tokens.eos_token, \n",
    "                                       token_type='eos_token', last=True),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c748e5c-88c8-41dc-9fe2-6c44219d9f6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just in case\n",
    "dataset.save_to_disk(config.path.dataset.just_before_encoding_dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35273e3d",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212135ba-62d9-4dd4-b6fe-fd9985483bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.just_before_encoding_dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a06bb-0af4-4bba-92a1-24b62bee425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.train.use_context = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622334a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ends = list(config.train.sentence_limits if config.train.sentence_limits else ['.', '!', '?', ';', '_'])\n",
    "dataset = dataset.map(\n",
    "        lambda examples: cleanup_stream(examples, separator='... ', add_context=config.train.use_context, ends=ends),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa787b07-a336-4c41-86a8-cd860edc6f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['train'][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f0d14",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5a132",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.path.tokenizer.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6225ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(lambda examples: encode_stream(examples, tokenizer), \n",
    "                              batched=True, \n",
    "                              num_proc=NUM_PROC, \n",
    "                              remove_columns=[\"stream\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f59655",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(config.path.dataset.prepared_dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd91a2-5bc0-4d44-81af-3097eb2c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.path.dataset.prepared_dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aae1db",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41121073-8b24-4392-bc5e-4eebdd600545",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(config.path.dataset.prepared_dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fb15d-d8e2-4c36-bfdb-21bbc1f63dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.path.dataset.prepared_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a1e-6ec9-43bd-be3d-b8068f19967f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b193d83-ddbe-48b4-85dd-2243c01ebf75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = encoded_dataset['train'][39]\n",
    "tkns = tokenizer.convert_ids_to_tokens(c['input_ids'])\n",
    "for i in range(len(c['input_ids'])):\n",
    "    #if c['token_type'][i] in ['T-11', 'time_sep']:\n",
    "     print(\"{:15} {:7} {:15} {:10}\".format(tkns[i], c['input_ids'][i], c['time'][i], c['token_type'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db9045-5123-4607-8f12-6df13c6e6558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(c['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7e953-d3ef-40a8-823b-f80e0b0b8fbe",
   "metadata": {},
   "source": [
    "## Prepare the DS for the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f06a6-b27c-4d75-8941-d3abf0f88ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medgpt.tokenizers.utils import pack_text, create_labels, pack_examples\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.path.tokenizer.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3bf7a-71ba-49a8-8f83-1f04c32c685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = encoded_dataset['test']\n",
    "dataset = dataset.remove_columns(['patient_id', 'token_type', 'time'])\n",
    "\n",
    "for max_len in [512, 1024, 2048]:\n",
    "    config.train.max_timeline_len = max_len\n",
    "    \n",
    "    # Do test if needed\n",
    "    _dataset = dataset.map(\n",
    "        lambda examples: pack_text(examples, max_len=config.train.max_timeline_len),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=1,\n",
    "    )\n",
    "    # Create labels for supervised training\n",
    "    cuis = pickle.load(open(config.path.dataset.cuis_in_text, 'rb'))\n",
    "    cui_ids = set(tokenizer.convert_tokens_to_ids([c for c in cuis]))\n",
    "    _dataset = _dataset.map(\n",
    "        lambda examples: create_labels(examples, config, cui_ids),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=8,\n",
    "    )\n",
    "\n",
    "    name = config.path.dataset.metrics_folder.split(\"/\")[-2][:-7] + 'test_set.hf'\n",
    "    _dataset.save_to_disk(config.path.dataset.test_sets_folder + name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
