{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd237f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"/data/zeljko/projects/medgpt/\")\n",
    "sys.path.insert(0, \"/data/zeljko/projects/MedCAT/\")\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/data/zeljko/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/data/zeljko/.cache/huggingface\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a81d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, Trainer, TrainingArguments, AutoTokenizer, pipeline, GPT2Tokenizer, LlamaForCausalLM, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "from medgpt.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "from medgpt.models.utils import add_cuis_to_model_and_tokenizer\n",
    "from medgpt.tokenizers.utils import pack_text, create_labels, pack_examples\n",
    "import re\n",
    "import pickle\n",
    "from medcat.cat import CAT\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import random\n",
    "import math\n",
    "from medgpt.config import Config\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from medgpt.metrics.next_concept_prediction import ComputePrecisionHF\n",
    "from medgpt.datasets.data_collator import CollataAndPad\n",
    "import collections\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47e6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(yaml_path='/home/ubuntu/projects/medgpt/configs/mimic-mistral.yaml', \n",
    "                extra_yaml_paths=['/home/ubuntu/projects/medgpt/configs/mimic-seq-len-4096.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6dd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = CAT.load_model_pack(config.path.cat, meta_cat_config_dict={'general': {'device': config.cat.meta.device}})\n",
    "cdb = cat.cdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cd0b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the just saved models\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.path.tokenizer.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe177c-ee11-410c-824b-82e8011f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.prepared_risk_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d723cf-59f7-40c7-b9bb-735fc022cdea",
   "metadata": {},
   "source": [
    "# Automatic risk validation | fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c827c-8dd4-49a6-b85c-e9b71ef6b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "  api_key = \"##\",  \n",
    "  api_version = \"2024-02-15-preview\",\n",
    "  azure_endpoint = \"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76634f-8742-4c26-8c6b-c8e075cb95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = '''\n",
    "You are now playing the role of a medical doctor taking an exam,\n",
    "your goal is to be as accurate as possible and make sure you do not make any mistakes. If you\n",
    "are unsure about something, think step by step and then answer. You have the follow the instructions\n",
    "precisely.'''\n",
    "sc2 = '''Your first task is to compare how many of the predicted disorders marked as `Predictions:` match the labels marked as `Labels:` in the input.\n",
    "Something is a match if it is approximately the same disorder (based on the definition of the disorder). \n",
    "For example `Diabetes` and `T1DM` are a match, T1DM and T2DM are types of `Diabetes`, i.e. they are more specific. The reverse is also fine, T1DM is a match for Diabetes.\n",
    "The output should be a json file formatted as follows: {'explanation': <your brief explanation>, 'number_of_matches': <number>}'''\n",
    "\n",
    "validation_prompt = '''Labels: {labels}\n",
    "Predictions: {predictions}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042ae15-62ab-47e4-bfbe-82289b94649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "def ask_openai(prompt, sc, sc2, model='gpt-4-turbo'):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sc},\n",
    "            {\"role\": \"system\", \"content\": sc2},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    message = None\n",
    "    if response.choices[0].finish_reason == 'stop':\n",
    "        message = response.choices[0].message.content\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638098e4-f944-4179-852a-141373263851",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "def ask_openai_json(prompt, sc, sc2, model='gpt-4-turbo'):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sc},\n",
    "            {\"role\": \"system\", \"content\": sc2},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    message = None\n",
    "    if response.choices[0].finish_reason == 'stop':\n",
    "        message = response.choices[0].message.content\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0c780-c61e-4305-a5d3-e74d1f7fc5cb",
   "metadata": {},
   "source": [
    "# Risk | Foresight test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77e9ea-07c7-46e9-a673-305e62d2feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medgpt.sight import Sight\n",
    "from medgpt.metrics.next_concept_prediction import ComputePrecisionHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f30dee-c244-44dd-891f-b9b6cb32c18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(config.path.trained_model_risk, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74330eee-0669-4a24-a039-f1ec0e88f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.prepared_risk_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be74a85-32f8-486f-9c4c-462131d985f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sight = Sight(tokenizer=tokenizer, model=model, device=model.device, cat=cat)\n",
    "token_type2tokens = pickle.load(open(config.path.tokenizer.token_type2tokens, 'rb'))\n",
    "id2tkn = {v:k for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10954d7-db2d-41ba-bcea-711a4d2725d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pipeline(model=model, tokenizer=tokenizer, task='text-generation')#, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f200c-0fde-4fc7-b591-0c680734f42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_ds = [('index', 'past', 'labels', 'preds', 'correct', 'wrong', 'fuzzy_matches')]\n",
    "limit =  5\n",
    "cors = []\n",
    "icors = []\n",
    "all_diseases = []\n",
    "n_pts = 800\n",
    "at_least_one = 0\n",
    "\n",
    "u_gen = False # This switches between using sight or generation\n",
    "\n",
    "for ind, pt in enumerate(dataset['test']):\n",
    "    # Find the end\n",
    "    end = len(pt['input_ids']) - 1 # I know, not the nicest way to do things, it was late\n",
    "    for i in range(len(pt['input_ids'])):\n",
    "        if pt['input_ids'][end - i] == 29901:#28747: # The last token before risks\n",
    "            end = end - i + 1\n",
    "            break\n",
    "    #print(len([x for x in tokenizer.convert_ids_to_tokens(pt['input_ids'][0:end]) if x in cat.cdb.cui2names]))\n",
    "    labels = tokenizer.convert_ids_to_tokens(pt['input_ids'][end:-1])\n",
    "    if not u_gen:\n",
    "        preds = [x[0] for x in sight.next_concepts(input_ids=pt['input_ids'][0:end], type_ids=['T-11'], n=limit, token_type2tokens=token_type2tokens, tkn2id=tokenizer.vocab, id2token=id2tkn)]\n",
    "    else:\n",
    "        preds = tokenizer.convert_ids_to_tokens(model.generate(torch.tensor([pt['input_ids'][0:end]]).to(model.device), max_length=len(pt['input_ids'][0:end]) + limit).detach().to('cpu')[0][-limit:].tolist())\n",
    "\n",
    "    if len(labels) >= limit:\n",
    "        cor = []\n",
    "        icor = []\n",
    "        all_diseases.extend(labels)\n",
    "        for p in preds:\n",
    "            if p in labels:\n",
    "                cor.append(p)\n",
    "            else:\n",
    "                icor.append(p)\n",
    "        cors.append(cor)\n",
    "        icors.append(icor)\n",
    "\n",
    "        if len(cor) > 0:\n",
    "            at_least_one += 1\n",
    "    \n",
    "        out_ds.append((ind, \n",
    "                       '\\n'.join([cdb.get_name(tkn) for tkn in tokenizer.convert_ids_to_tokens(pt['input_ids'][0:end]) if tkn in cdb.cui2names]),\n",
    "                       '\\n'.join([cdb.get_name(x) for x in labels]), \n",
    "                       '\\n'.join([cdb.get_name(x) for x in preds]),\n",
    "                       '\\n'.join([cdb.get_name(x) for x in cor]), \n",
    "                       '\\n'.join([cdb.get_name(x) for x in icor]),\n",
    "                       ''))\n",
    "        print(len(out_ds)-1, len(cor), len(icor))\n",
    "        if len(out_ds) > n_pts:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7126b-a5bd-4a87-ad66-11485a883415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=out_ds[0], data=out_ds[1:])\n",
    "df.to_csv(\"./metrics/fs2_risk_validation_{}.csv\".format(config.id), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb12756-b60f-4017-8c1a-7bd6f2c40d0d",
   "metadata": {},
   "source": [
    "### Do the fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3ef99-8ef7-47b0-b0ad-5a52bc697184",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "last_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869302b-ea64-43cc-a541-1a9997fc5751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(last_i, len(df)):\n",
    "    try:\n",
    "        o = json.loads(ask_openai_json(validation_prompt.format(labels=df['labels'][i], predictions=df['preds'][i]), model='gpt-4-turbo'))\n",
    "        if 'number_of_matches' in o:\n",
    "            output.append(o)\n",
    "            print(i, o)\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "    last_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbebe7-a28b-4494-b039-90efad7ed322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(output)\n",
    "len(df2[df2.number_of_matches > 0]) / len(df2), len(df2[df2.number_of_matches > 1]) / len(df2), len(df2[df2.number_of_matches > 2]) / len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b30fcd-6724-4d2b-bafc-d914f2537864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"./metrics/fs2_risk_validation_via-gpt_{}.csv\".format(config.id), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c5ff7-9a45-4100-b9a0-2cb9beed839b",
   "metadata": {},
   "source": [
    "# Risk | GPT-4 via Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9e1b5-6a45-404d-857e-d2391b8727fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.prepared_risk_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14a8b2-0304-46b1-84a9-9a25370279ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = '''\n",
    "You are now playing the role of a medical doctor taking an exam,\n",
    "your goal is to be as accurate as possible and make sure you do not make any mistakes. If you\n",
    "are unsure about something, think step by step and then answer. You have the follow the instructions\n",
    "precisely.'''\n",
    "sc2 = '''Your first question in this medical quiz will consist of a patient history, your goal is to predict {limit} specific disorders\n",
    "the patient is at risk for in the next month. Please take care that the disorders you are predicting cannot be part of the patient's past. They\n",
    "have to be new disorders that will most likely affect the patient in the next month. You have to predict specific disorders, for example: you should never say \"pulmunary problems\"\n",
    "as this is not a specific disorder, but you can say \"pneumonia\" as that is a specific disorder.'''\n",
    "\n",
    "prompt = '''{history}\n",
    "\n",
    "Given the above patient history, what {limit} specific new disorders is this patient at risk for in the next month?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305180b-0388-44d9-a2da-6c68d2bcbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "def ask_openai(prompt, sc, sc2, model='gpt-4-turbo'):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sc},\n",
    "            {\"role\": \"system\", \"content\": sc2},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    message = None\n",
    "    if response.choices[0].finish_reason == 'stop':\n",
    "        message = response.choices[0].message.content\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f4eb8-6622-4118-8207-ee29007e4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ds = [('index', 'past', 'labels', 'preds', 'correct-top-5', 'wrong-top-5', 'correct-top-3', 'wrong-top-3')]\n",
    "limit =  5\n",
    "cors = []\n",
    "icors = []\n",
    "n_pts = 1000\n",
    "past = []\n",
    "last_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a99db-0a9c-4965-bd90-d234a3ba44a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(last_i, len(dataset['test'])):\n",
    "    _text = tokenizer.convert_tokens_to_string([' ' + cat.cdb.get_name(x) if x in cat.cdb.cui2names else x for x in tokenizer.convert_ids_to_tokens(dataset['test'][i]['input_ids'])])[4:-4]\n",
    "    try:\n",
    "        text, labels = [x.strip() for x in _text.split('In the next month the patient is at risk of:')]\n",
    "        labels = [x.strip() for x in labels.split('(disorder)') if x]\n",
    "        t = prompt.format(history=text, limit=limit)\n",
    "    \n",
    "        if len(labels) >= limit:\n",
    "            response = ask_openai(t, sc, sc2, model='gpt-4-turbo')\n",
    "            #response = json.loads(ask_openai(t, sc, sc2, model='gpt-4-turbo'))\n",
    "            out_ds.append((i, \n",
    "                           text,\n",
    "                           '\\n'.join(labels), \n",
    "                           response,#'\\n'.join([x[0] for x in response['predictions']]),\n",
    "                           '', \n",
    "                           '',\n",
    "                           '',\n",
    "                           ''))\n",
    "            print(i, '; '.join(labels), response)\n",
    "            if len(out_ds) > n_pts:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Skip: \", i)\n",
    "    last_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aef509-41c9-4aa5-82d0-b5ef7c5a0447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=out_ds[0][:-1], data=out_ds[1:])\n",
    "df.to_csv(\"./metrics/fs2_risk_predictions_gpt-4-turbo.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f86cf-d082-4313-8b78-1f9366cec49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = '''\n",
    "You are now playing the role of a medical doctor taking an exam,\n",
    "your goal is to be as accurate as possible and make sure you do not make any mistakes. If you\n",
    "are unsure about something, think step by step and then answer. You have the follow the instructions\n",
    "precisely.'''\n",
    "sc2 = '''Your first task is to compare how many of the predicted disorders marked as `Predictions:` match the labels marked as `Labels:` in the input.\n",
    "Something is a match if it is approximately the same disorder (based on the definition of the disorder). \n",
    "For example `Diabetes` and `T1DM` are a match, T1DM and T2DM are types of `Diabetes`, i.e. they are more specific. The revrse is also fine, T1DM is match for Diabetes.\n",
    "The output should be a json file formatted as follows: {'explanation': <your brief explanation>, 'number_of_matches': <number>}'''\n",
    "\n",
    "validation_prompt = '''Labels: {labels}\n",
    "Predictions: {predictions}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cc572-694f-4f9a-aa4e-f38aa0c9a4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(df)):\n",
    "    msg = validation_prompt.format(labels=df['labels'][i], predictions=df['preds'][i])\n",
    "    o = json.loads(ask_openai_json(msg, sc, sc2, model='gpt-4-turbo'))\n",
    "    output.append(o)\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a938cb5-d66c-4b31-8338-5a97660e3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(output)\n",
    "len(df2[df2.number_of_matches > 0]) / len(df2), len(df2[df2.number_of_matches > 1]) / len(df2), len(df2[df2.number_of_matches > 2]) / len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54675be5-d636-4fae-a3c9-cc3e5c6a166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"./metrics/fs2_risk_validation_gpt-4-turbo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781c1df-90f6-4c9d-ac27-4bc0732cc096",
   "metadata": {},
   "source": [
    "# Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc91692-c112-4d11-b314-1ceb49f4b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "ds = dataset['test']\n",
    "for i in range(len(ds)):\n",
    "    _text = tokenizer.convert_tokens_to_string([' ' + cat.cdb.get_name(x) if x in cat.cdb.cui2names else x for x in tokenizer.convert_ids_to_tokens(dataset['test'][i]['input_ids'])])[4:-4]\n",
    "    parts = [x.strip() for x in _text.split('In the next month the patient is at risk of:')]\n",
    "    if len(parts) == 2:\n",
    "        labels = [x.strip() for x in parts[1].split('(disorder)') if x]\n",
    "        if len(labels) >= 5:\n",
    "            text = parts[0].strip()\n",
    "            data.append((text, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1219d-b3ea-4554-9be1-a74461fcac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a069b-8fdd-46ba-af4e-ee103258b553",
   "metadata": {},
   "source": [
    "# Test BioMistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef9852-86a3-4070-b04f-4d55e8c1b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, MistralForCausalLM\n",
    "import json\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\")\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\"BioMistral/BioMistral-7B\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cc454-bdf6-451e-ba22-792669e8b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mistral = pipeline(model=mistral_model, tokenizer=mistral_tokenizer, task='text-generation')#, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10dc404-1e01-4bfd-908c-4e718b88bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_prompt = '''<s>Please truthfully answer the following question. Please ensure that\n",
    "your choice is socially unbiased and positive. If you don’t know the answer to a question, \n",
    "please don’t share false information.\n",
    "\n",
    "<patient_history>\n",
    "{history}\n",
    "</patient_history>\n",
    "\n",
    "Given the above patient history, what {limit} specific new disorders is this patient at risk for in the next month? The answer is:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e0fee-72fd-4381-b266-69646ee8ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1ce85-4cc7-4589-96b5-197b86c0498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this 10 times or more, get an average and use that - outputs are a bit unpredictable\n",
    "limit = 5\n",
    "max_seq_len = 2048\n",
    "\n",
    "for ind in range(len(output) + 1, len(data)): # subset if needed to the 100 from the test set used by gpt-4\n",
    "    t = mistral_prompt.format(history=data[ind][0], limit=limit)\n",
    "    labels = data[ind][1]\n",
    "    if len(mistral_tokenizer(t)['input_ids']) < max_seq_len - 128 and len(labels) >= limit:\n",
    "        o = gen_mistral(t, max_length=max_seq_len, do_sample=True)\n",
    "        text_predictions = o[0]['generated_text'].split(\"The answer is:\")[1].strip() #.split(\".\")[0]   \n",
    "        if len(text_predictions) > 10: # Just make sure there is something, otherwise skip\n",
    "            text_labels = \", \".join(labels)\n",
    "    \n",
    "            _prompt = validation_prompt.format(labels=text_labels, predictions=text_predictions)\n",
    "            try:\n",
    "                o = json.loads(ask_openai_json(validation_prompt.format(labels=text_labels, predictions=text_predictions), model='gpt-4-1106-preview'))\n",
    "                print(ind, o)\n",
    "                if 'number_of_matches' in o:\n",
    "                    o['prompt'] = _prompt # so we have everything saved\n",
    "                    output.append(o)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74a4cd-767d-4be7-8a89-7bd446a8874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)\n",
    "len(df[df.number_of_matches > 0]) / len(df), len(df[df.number_of_matches > 1]) / len(df), len(df[df.number_of_matches > 2]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a20dcc-1e2e-4ccd-9ad2-aabde84114a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./metrics/fs2_risk_validation_biomistral.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a2a55-a6e9-496f-a748-3f830271e255",
   "metadata": {},
   "source": [
    "# Test MedAlpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8c59b-38ec-4912-942b-28cfdbef7f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "gen_med_llama = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"medalpaca/medalpaca-7b\")\n",
    "tokenizer_med_llama = AutoTokenizer.from_pretrained(\"medalpaca/medalpaca-7b\", model_max_length=2048) # 2048 was the one set in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7ab6b-9412-4e78-8173-7955f454ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "medalpaca_prompt = '''Context: {history}\n",
    "\n",
    "Question: Given the above patient history, what {limit} specific new disorders is this patient at risk for in the next month?\n",
    "\n",
    "Answer: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59df5f-26e0-4e63-ad7f-f6725fcb2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764d50b-b82e-4222-a2d2-4788531827a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit = 5\n",
    "max_seq_len = 2048\n",
    "for ind in range(len(output) + 1, len(data)):\n",
    "    t = medalpaca_prompt.format(history=data[ind][0], limit=limit)\n",
    "    labels = data[ind][1]\n",
    "    if len(tokenizer_med_llama(t)['input_ids']) < max_seq_len - 128 and len(labels) >= limit:\n",
    "        o = gen_med_llama(t, max_length=max_seq_len, do_sample=True)\n",
    "        text_predictions = o[0]['generated_text'].split(\"Answer:\")[1].strip() #.split(\".\")[0]   \n",
    "        if len(text_predictions) > 10: # Just make sure there is something, otherwise skip\n",
    "            text_labels = \", \".join(labels)\n",
    "    \n",
    "            _prompt = validation_prompt.format(labels=text_labels, predictions=text_predictions)\n",
    "            try:\n",
    "                o = json.loads(ask_openai_json(validation_prompt.format(labels=text_labels, predictions=text_predictions), model='gpt-4-1106-preview'))\n",
    "                print(ind, o)\n",
    "                if 'number_of_matches' in o:\n",
    "                    o['prompt'] = _prompt # so we have everything saved\n",
    "                    output.append(o)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aabce2-788e-4f04-83e0-5bbf2b7b1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)\n",
    "df.to_csv(\"./metrics/fs2_risk_validation_medalpaca.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc1239-a169-4803-9996-e5e0d5a295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.number_of_matches > 0]) / len(df), len(df[df.number_of_matches > 1]) / len(df), len(df[df.number_of_matches > 2]) / len(df), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392a575-2d19-4a89-bcd9-8ad26104a498",
   "metadata": {},
   "source": [
    "# Test MediTron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b93f3e-c200-4210-93a7-33605ad79d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "gen_meditron = pipeline(\"text-generation\", model=\"epfl-llm/meditron-7b\", tokenizer=\"epfl-llm/meditron-7b\")\n",
    "tokenizer_meditron = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0296d-9386-4200-a07d-d9f6617f8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are a medical doctor answering real-world medical entrance exam questions. Based\n",
    "on your understanding of basic and clinical science, medical knowledge, and mechanisms\n",
    "underlying health, disease, patient care, and modes of therapy, answer the question below given the following context:\n",
    "\n",
    "<patient history>\n",
    "{history}\n",
    "</patient history>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f5e9b-5e35-4405-963a-cccb2be516b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = 'Given the above patient history, what 5 specific new disorders is this patient at risk for in the next month?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef47b5e-8d4c-4d8a-9166-200f38c5ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "meditron_prompt = '''<|im_start|>system\n",
    "{system}<|im_end|>\n",
    "<|im_start|>question\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>answer '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14fd21-3815-4e9e-9006-9709392fb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "last_i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539976e-8f65-40a4-894b-faa461dcd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "o[0]['generated_text'].split(\"<|im_start|>answer\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803d048-4e86-4434-b023-39332c36becd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit = 5\n",
    "max_seq_len = 2048\n",
    "for ind in range(last_i + 1, len(data)):\n",
    "    t = meditron_prompt.format(system=system_prompt.format(history=data[ind][0]), prompt=user_prompt)\n",
    "    labels = data[ind][1]\n",
    "    if len(tokenizer_meditron(t)['input_ids']) < max_seq_len - 128 and len(labels) >= limit:\n",
    "        o = gen_meditron(t, max_length=max_seq_len, do_sample=True)\n",
    "        text_predictions = o[0]['generated_text'].split(\"<|im_start|>answer\")[1].strip() #.split(\".\")[0]   \n",
    "        if len(text_predictions) > 10: # Just make sure there is something, otherwise skip\n",
    "            text_labels = \", \".join(labels)\n",
    "    \n",
    "            _prompt = validation_prompt.format(labels=text_labels, predictions=text_predictions)\n",
    "            try:\n",
    "                o = json.loads(ask_openai_json(validation_prompt.format(labels=text_labels, predictions=text_predictions), model='gpt-4-1106-preview'))\n",
    "                print(ind, o)\n",
    "                if 'number_of_matches' in o:\n",
    "                    o['prompt'] = _prompt # so we have everything saved\n",
    "                    output.append(o)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    last_i = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ccbff-c011-4ded-913b-ef852def3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)\n",
    "df.to_csv(\"./metrics/fs2_risk_validation_meditron.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541bf8f-7b90-4316-ac0f-51ebcdf39c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.number_of_matches > 0]) / len(df), len(df[df.number_of_matches > 1]) / len(df), len(df[df.number_of_matches > 2]) / len(df), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59875b-d85e-4928-b6e7-60f88b6fea1f",
   "metadata": {},
   "source": [
    "# Test next concept prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc0ae3-bc71-47d2-9e7a-1014179adf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f79c11-a4ae-4c84-9a6a-d79788bdbe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(yaml_path='/home/ubuntu/projects/medgpt/configs/mimic-mistral.yaml', \n",
    "                extra_yaml_paths=['/home/ubuntu/projects/medgpt/configs/mimic-seq-len-4096.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da523f-4095-493e-95cb-1279c1583476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(config.path.trained_model, use_flash_attention_2=False, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.path.tokenizer.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421b037-5b6c-4bb1-a559-f9d431e0eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(config.path.dataset.prepared_dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a3d78-9fbf-46d0-b47a-d454f4bc9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdc459-4f2b-4b9c-9479-80566d72ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_to_use = dataset['test']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114051a1-0fcd-494e-ad7d-01a923815022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels, if not added loss makes no sense but metrics are still fine\n",
    "cuis = pickle.load(open(config.path.dataset.cuis_in_text, 'rb'))\n",
    "cui_ids = set(tokenizer.convert_tokens_to_ids([c for c in cuis]))\n",
    "test_set_to_use = test_set_to_use.map(\n",
    "    lambda examples: create_labels(examples, config, cui_ids),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae9b74-4b54-4293-b282-69600a00985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(metrics_data=None, test_set_to_use=None, trainer=None, m_file=None, f_name=None):\n",
    "    size = 20\n",
    "    for i in range(int(math.ceil(len(test_set_to_use) / size))):\n",
    "        _dataset = Dataset.from_dict(test_set_to_use[i*size:(i+1)*size])\n",
    "        compute_metrics.time_data = _dataset['time']\n",
    "        compute_metrics.type_data = _dataset['token_type']\n",
    "        if len(_dataset):\n",
    "            p = trainer.predict(_dataset)\n",
    "            metrics_data = compute_metrics(p, metrics_data)['metrics_data']\n",
    "    m_file.write(\"{}, {}, {}, {}, {}, {}, {}\\n\".format(f_name, metrics_data['precision']['all'], \n",
    "                                 metrics_data['precision']['new'], \n",
    "                                 metrics_data['precision']['old'],\n",
    "                                 metrics_data['recall']['all'],\n",
    "                                 metrics_data['recall']['new'],\n",
    "                                 metrics_data['recall']['old']))\n",
    "    print(f_name,\n",
    "          metrics_data['precision']['all'], \n",
    "          metrics_data['precision']['new'], \n",
    "          metrics_data['precision']['old'],\n",
    "          metrics_data['recall']['all'],\n",
    "          metrics_data['recall']['new'],\n",
    "          metrics_data['recall']['old'])\n",
    "    with open(f_name, 'wb') as f:\n",
    "        pickle.dump(metrics_data, f)\n",
    "\n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88cbae-e5a6-4ad3-b00a-ad70a95af8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type2tokens = pickle.load(open(config.path.tokenizer.token_type2tokens, 'rb'))\n",
    "id2tkn = {v:k for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e4db6-4605-4769-8cd8-bf4d4c0f11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = set(token_type2tokens.keys())\n",
    "all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660175d-62e4-4887-9d56-6285c4841277",
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = config.train.hf_training_arguments.to_dict()\n",
    "# Set the dynamic dir for output\n",
    "targs['output_dir'] = config.path.dataset.hf_output_folder\n",
    "training_args = TrainingArguments(**targs)\n",
    "dc = CollataAndPad(max_seq_len=config.train.max_timeline_len, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=None,\n",
    "    compute_metrics=None,\n",
    "    data_collator=dc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd057ff-1694-4eae-a648-a28468b33e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_file = open(\"./metrics/summary-mistral.txt\", 'a', buffering=1)\n",
    "#m_file.write(\"file_name, precision all, precision new, precision old, recall all, recall new, recall old\\n\")\n",
    "\n",
    "for types in [{'T-55'}, {'T-18'}, {'T-39'}]: #all_types, {'T-11'}, \n",
    "    _types = list(types)[0] if len(types) == 1 else 'all_types'\n",
    "    for timerange in [30, 365, 1000000]:\n",
    "        compute_metrics = ComputePrecisionHF(id2tkn, \n",
    "                                         prediction_scope='time_range', \n",
    "                                         topk=1, # 1, 5, 10\n",
    "                                         start=0, # 0, 10, 20, 50, 100\n",
    "                                         return_all_metrics=True, \n",
    "                                         batch_size=1000, \n",
    "                                         select_token_types=types,\n",
    "                                         type_data=test_set_to_use['token_type'],\n",
    "                                         token_type2tokens=token_type2tokens,\n",
    "                                         time_data=test_set_to_use['time'], \n",
    "                                         time_range=timerange*24*60*60, #30, 365, 1000000\n",
    "                                         ignore_label_status=False,\n",
    "                                         min_time_left=24*60*60)\n",
    "        f_name = f\"./metrics/mistral-start-0_topk-1_time_range-{timerange}_types-{_types}.pickle\"\n",
    "        get_metrics(None, test_set_to_use, trainer, m_file, f_name)\n",
    "\n",
    "    for topk in [5, 10]:\n",
    "        compute_metrics = ComputePrecisionHF(id2tkn, \n",
    "                                         prediction_scope='time_range', \n",
    "                                         topk=topk, # 1, 5, 10\n",
    "                                         start=0, # 0, 10, 20, 50, 100\n",
    "                                         return_all_metrics=True, \n",
    "                                         batch_size=1000, \n",
    "                                         select_token_types=types,\n",
    "                                         type_data=test_set_to_use['token_type'],\n",
    "                                         token_type2tokens=token_type2tokens,\n",
    "                                         time_data=test_set_to_use['time'], \n",
    "                                         time_range=30*24*60*60, #30, 365, 1000000\n",
    "                                         ignore_label_status=False,\n",
    "                                         min_time_left=24*60*60)\n",
    "        f_name = f\"./metrics/mistral-start-0_topk-{topk}_time_range-30_types-{_types}.pickle\"\n",
    "        get_metrics(None, test_set_to_use, trainer, m_file, f_name)\n",
    "m_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82655cc8-22be-4141-9d4d-6d6cebf816ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./metrics/summary-mistral.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfd3af-fdb0-4e2a-a922-ab44ed35c8de",
   "metadata": {},
   "source": [
    "# Test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edece360-a06e-487c-a728-1c87a33ec227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medgpt.sight import Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb61ba-0b09-404d-945e-593adc361a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sight = Sight(tokenizer=tokenizer, model=model, device=model.device, cat=cat)\n",
    "token_type2tokens = pickle.load(open(config.path.tokenizer.token_type2tokens, 'rb'))\n",
    "id2tkn = {v:k for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b0f16-b148-4620-808c-4cc77ad9652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1edaf-6958-4d82-80e9-351a09f828d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in sight.next_concepts(t, type_ids=None, n=50, token_type2tokens=token_type2tokens, tkn2id=tokenizer.vocab, id2token=id2tkn):\n",
    "    print(x[1], cat.cdb.get_name(x[0].strip()), x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df07bc-3276-427a-8b93-0b8db9de371a",
   "metadata": {},
   "source": [
    "# Get the metrics into the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d05472-ec17-4bcc-941c-d3512355089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./metrics/summary-mistral.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee716a94-865d-47b4-8ac7-608b6db9e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"topk-(?P<topk>\\d+)_time_range-(?P<time_range>\\d+)_types-(?P<types>.+?)\\.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1972799-a057-44da-9a01-c0f37e14cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = []\n",
    "t = []\n",
    "t_days = []\n",
    "for val in df.file_name:\n",
    "    params = re.search(pattern, val).groupdict()\n",
    "    at.append(params['topk'])\n",
    "    if params['types'] == 'all_types':\n",
    "        t.append('All')\n",
    "    elif params['types'] == 'T-11':\n",
    "        t.append('Disorders')\n",
    "    elif params['types'] == 'T-18':\n",
    "        t.append('Findings')\n",
    "    elif params['types'] == 'T-55':\n",
    "        t.append('Substances')\n",
    "    elif params['types'] == 'T-39':\n",
    "        t.append('Procedures')\n",
    "    if params['time_range'] == '1000000':\n",
    "        t_days.append('inf')\n",
    "    else:\n",
    "        t_days.append(params['time_range'])\n",
    "df['@'] = at\n",
    "df['Type'] = t\n",
    "df['T - days'] = t_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ff3b6-b548-45b7-a0ad-319abe981404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_new = []\n",
    "_recurring = []\n",
    "for i, row in df.iterrows():\n",
    "    _new.append(\"{:.2f}/{:.2f}\".format(row[' precision new'], row[' recall new']))\n",
    "    _recurring.append(\"{:.2f}/{:.2f}\".format(row[' precision old'], row[' recall old']))\n",
    "df['New P/R'] = _new\n",
    "df['Recurring P/R'] = _recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200adfc4-7fa3-4ad0-a712-61334a0aa824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df[['Type', 'T - days', '@', 'New P/R', 'Recurring P/R']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e610fa8-868c-4c1c-8a2b-90a091eadf09",
   "metadata": {},
   "source": [
    "# Top and Bottom 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3db270-a239-4fc2-bb24-eb36ac7d2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open(\"./metrics/start-0_topk-1_time_range-30_types-T-11.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc639b0-57c8-46f6-9fac-6da1bf58c855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a19fa-b89b-43df-ae81-aae19d99471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = {}\n",
    "for cui in m['positives']['new'].keys():\n",
    "    prec[cui] = (m['positives']['new'][cui] / (m['positives']['new'][cui] + m['negatives']['new'].get(cui, 0)), m['positives']['new'][cui], m['negatives']['new'].get(cui, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701cbfb-cb1b-4487-8c51-8aa98d1daf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = sorted(prec.items(), key=lambda x: x[1][0], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08b873-7ef4-43e2-97f7-c1e6fcd82876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(cat.cdb.get_name(x[0]), x[0], x[1][0], x[1][1], x[1][2]) for x in sorted_data if x[1][1] > 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
